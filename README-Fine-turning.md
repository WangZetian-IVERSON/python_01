# Python Learning Projects--WangZetian ğŸ

æ¬¢è¿æ¥åˆ°æˆ‘çš„ Python å­¦ä¹ é¡¹ç›®ä»“åº“ï¼è¿™é‡ŒåŒ…å«äº†æˆ‘åœ¨å­¦ä¹  Python è¿‡ç¨‹ä¸­å®Œæˆçš„å„ç§é¡¹ç›®å’Œç»ƒä¹ ã€‚(welcome come to my repository)

1:![Music Particle Effects](music_particle_effects.gif)

## ğŸµ ç‰¹è‰²é¡¹ç›®

### éŸ³ä¹ç²’å­ç‰¹æ•ˆç³»ç»Ÿ
ä¸€ä¸ªåŸºäºéŸ³é¢‘é©±åŠ¨çš„å®æ—¶ç²’å­ç‰¹æ•ˆå¯è§†åŒ–é¡¹ç›®ï¼Œå±•ç¤ºäº†éŸ³ä¹ä¸è§†è§‰è‰ºæœ¯çš„ç»“åˆã€‚(music&art)

**ä¸»è¦ç‰¹æ€§ï¼š**
- ğŸµ Real-time audio analysis and beat detection
- ğŸ¨ Particle system
- âš¡ 60FPS Smooth rendering
- ğŸ® Interactive control
- ğŸ“± You can press any key on the keyboard to create a beat

**PJ locationï¼š** `week02/music_particle_effects/`

**quick startï¼š**
```bash
cd week02/music_particle_effects
pip install pygame
python quick_start.py
```
**Main languageï¼š** Python 3.8+

**Main repoï¼š**
- `pygame` - Game development and real-time rendering
- `matplotlib` - Data visualization
- `librosa` - Audio analysis
- `numpy` - Numerical analysis
- `requests` - Network requests
- `BeautifulSoup` - Web page parsing
- `FastAPI` - Web API  exploitation
- `Docker` - Containerized deployment

## ğŸš€ How

1. **clone**
   ```bash
   git clone https://github.com/WangZetian-IVERSON/python_01.git
   cd python_01
   ```

2. **set environment**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # Linux/Mac
   # æˆ–
   .venv\Scripts\activate  # Windows
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run**
   ```bash
   # ä¾‹å¦‚è¿è¡ŒéŸ³ä¹ç²’å­ç‰¹æ•ˆ
   cd week02/music_particle_effects
   python quick_start.py
   ```

## devoteğŸ¤ 
Suggestions and improvements are welcome! If you have any ideas or find an issue, please create an issue or submit a pull request.

## ğŸ“licence

Only studyã€‚

---

# 2ï¼š LM Studio DeepSeek Chatbot
<img width="2550" height="1397" alt="image" src="https://github.com/user-attachments/assets/1165508b-def5-4244-82f7-025c4d1ca82f" />

ä¸€ä¸ªåŠŸèƒ½å®Œå–„çš„AIèŠå¤©æœºå™¨äººï¼ŒåŸºäºStreamlitæ„å»ºï¼Œæ”¯æŒæœ¬åœ°å’Œäº‘ç«¯åŒæ¨¡å¼ã€‚

## âœ¨ ç‰¹æ€§

- ğŸ¤– **åŒæ¨¡å¼æ”¯æŒ**: è‡ªåŠ¨åˆ‡æ¢æœ¬åœ°LM Studioæˆ–DeepSeekäº‘ç«¯API ï¼ˆAutomatically switch between local LM Studio or DeepSeek cloud APIsï¼‰
- ğŸ’¬ **æ™ºèƒ½å¯¹è¯**: æ”¯æŒæµå¼å“åº”å’Œæ€è€ƒè¿‡ç¨‹æ˜¾ç¤º ï¼ˆSupport streaming response and thought process displayï¼‰
- ğŸ“š **å†å²ç®¡ç†**: ä¾§è¾¹æ å¯¹è¯å†å²ï¼Œæ”¯æŒæ–°å»ºã€åˆ é™¤ã€åˆ‡æ¢å¯¹è¯ ï¼ˆSidebar conversation history, which supports creating, deleting, and switching conversationsï¼‰
- ğŸ¨ **ç¾è§‚ç•Œé¢**: æ·±è‰²ä¾§è¾¹æ +ç™½è‰²æ–‡å­—ï¼Œä¸»å†…å®¹åŒºé»‘è‰²æ–‡å­— ï¼ˆDark sidebar + white text, black text in the main content areaï¼‰
- ğŸ”„ **å®æ—¶æ›´æ–°**: æµå¼æ˜¾ç¤ºAIå›ç­”è¿‡ç¨‹ ï¼ˆStreaming the AI answering processï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

### å¯åŠ¨åº”ç”¨
```bash
streamlit run lmstudio_chatbot.py

*æŒç»­å­¦ä¹ ï¼Œä¸æ–­è¿›æ­¥ï¼* ğŸŒŸ
## ğŸ“§ about

- GitHub: [@WangZetian-IVERSON](https://github.com/WangZetian-IVERSON)
- é¡¹ç›®ä»“åº“: [python_01](https://github.com/WangZetian-IVERSON/python_01)

---

## unsloth â€” qwen2.5 7B Fine-Tuning

This section documents a LoRA micro-finetuning run performed locally on a Qwen2.5-7B-Instruct base model. It was included here so collaborators can reproduce training and inference steps.

## æ¦‚è¿°ï¼ˆOverviewï¼‰
æœ¬æ¬¡è®­ç»ƒä»¥ Qwen2.5-7B-Instruct ä¸ºåŸºç¡€æ¨¡å‹ï¼Œé‡‡ç”¨ LoRAï¼ˆä½ç§©é€‚é…ï¼‰è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼‰ï¼Œæ•°æ®æºä¸º Hugging Face çš„ FreedomIntelligence/medical-o1-reasoning-SFTï¼ˆå·²æ¸…æ´—å¹¶å¯¼å‡ºä¸ºæœ¬åœ° JSONï¼‰ã€‚ç›®æ ‡æ˜¯åœ¨æœ‰é™æ˜¾å­˜ä¸‹å¾—åˆ°å¯ç”¨çš„ LoRA é€‚é…å™¨ï¼Œä¾¿äºåç»­æ¨ç†åŠ¨æ€åŠ è½½æˆ–ç¦»çº¿åˆå¹¶ã€‚ï¼ˆThis run fine-tuned Qwen2.5-7B-Instruct using LoRA for SFT on the FreedomIntelligence/medical-o1-reasoning-SFT dataset, which was preprocessed and saved as local JSON.ï¼‰

## ç¯å¢ƒä¸æ–‡ä»¶ï¼ˆEnvironment & Filesï¼‰
- GPU: NVIDIA GeForce RTX 4090ï¼ˆçº¦ 24GB VRAMï¼Œå•å¡ï¼‰ã€‚(GPU: NVIDIA GeForce RTX 4090 (~24GB VRAM, single card).)
- Conda ç¯å¢ƒå‰ç¼€: `/root/autdl-tmp/conda-envs/unsloth`ã€‚ï¼ˆConda env prefix: `/root/autdl-tmp/conda-envs/unsloth`ï¼‰
- åŸºç¡€æ¨¡å‹ï¼ˆBase modelï¼‰: `/root/autdl-tmp/modelscope/Qwen-Qwen2.5-7B-Instruct/...`ã€‚ï¼ˆBase model path.ï¼‰
- LoRA é€‚é…å™¨ä¸ tokenizerï¼ˆLoRA adapter & tokenizerï¼‰: `/root/autdl-tmp/unsloth/lora_model`ã€‚ï¼ˆLoRA adapter & tokenizer path.ï¼‰
- æœ¬åœ°è®­ç»ƒæ•°æ®ï¼ˆLocal datasetï¼‰: `filtered_medical_o1_sft_Chinese.json`ã€‚ï¼ˆLocal training data JSON.ï¼‰

## å…³é”®é…ç½®ï¼ˆKey Configsï¼‰
- LoRA: r=16, alpha=16ï¼›target modules ä»¥ q/k/v/o/gate/up/down ä¸ºä¸»ã€‚ï¼ˆLoRA params: r=16, alpha=16; target modules include q/k/v/o/gate/up/down.ï¼‰
- æœ€å¤§åºåˆ—é•¿åº¦: 8196ã€‚ï¼ˆSeq length: 8196.ï¼‰
- è®­ç»ƒè®¾ç½®: per-device batch=2ï¼Œgradient_accumulation=4ï¼Œepochs=1ï¼Œlearning_rate=1e-5ã€‚ï¼ˆTraining: per-device batch=2, grad_accum=4, epochs=1, lr=1e-5.ï¼‰
- ä½¿ç”¨ TRL çš„ SFTTrainer é©±åŠ¨è®­ç»ƒï¼Œå°½é‡é‡‡ç”¨ 4-bit åŠ è½½ï¼ˆbitsandbytesï¼‰ä»¥èŠ‚çœæ˜¾å­˜ã€‚ï¼ˆTraining used TRL's SFTTrainer and attempted 4-bit loading via bitsandbytes to save memory.ï¼‰

## ç»“æœé€Ÿè®°ï¼ˆResultsï¼‰
- å•è½®è®­ç»ƒå®Œæˆï¼Œè®­ç»ƒæ—¥å¿—æœ«æœŸ train_loss â‰ˆ 1.26ã€‚ï¼ˆSingle-epoch finished; final train_loss â‰ˆ 1.26.ï¼‰
- LoRA é€‚é…å™¨å·²ä¿å­˜äº `lora_model/`ï¼Œå¯åœ¨çº¿åŠ è½½æˆ–ç¦»çº¿åˆå¹¶ä¸ºå®Œæ•´æƒé‡ä»¥åŠ é€Ÿæ¨ç†å¯åŠ¨ã€‚ï¼ˆLoRA adapter saved to `lora_model/`; it can be loaded at runtime or merged offline into full weights to speed up startup.ï¼‰

## å…³é”®ä»£ç ç‰‡æ®µï¼ˆç¤ºæ„ / Key snippetsï¼‰
ä»¥ä¸‹ä»£ç ä¸ºç¤ºæ„æ‘˜å½•ï¼Œå®Œæ•´å®ç°è§ `FineTuning.py`ã€‚ï¼ˆThe following snippets are illustrative extracts; see `FineTuning.py` for full implementation.ï¼‰

åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆç¤ºæ„ / Load base modelï¼‰ï¼šï¼ˆChinese first, then Englishï¼‰
```python
from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name='/root/autdl-tmp/modelscope/Qwen-Qwen2.5-7B-Instruct/...',
    load_in_4bit=True,
    device_map='cuda:0',
    local_files_only=True,
    trust_remote_code=True,
)
```

åº”ç”¨ LoRAï¼ˆç¤ºæ„ / Apply LoRAï¼‰ï¼š
```python
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"],
    lora_alpha=16,
)
```

ä¿å­˜é€‚é…å™¨ï¼ˆç¤ºæ„ / Save adapterï¼‰ï¼š
```python
model.save_pretrained('lora_model')
tokenizer.save_pretrained('lora_model')
```

æ¨ç†æ—¶åŠ è½½ LoRAï¼ˆç¤ºæ„ / Load LoRA for inferenceï¼‰ï¼š
```python
from transformers import AutoModelForCausalLM
from peft import PeftModel

base = AutoModelForCausalLM.from_pretrained(base_path, local_files_only=True, trust_remote_code=True)
model = PeftModel.from_pretrained(base, '/root/autdl-tmp/unsloth/lora_model', local_files_only=True)
```

## å¤‡æ³¨ï¼ˆNotesï¼‰
å®Œæ•´å®ç°ä¸è®­ç»ƒç»†èŠ‚ä½äº `FineTuning.py`ï¼›å¦‚éœ€æˆ‘æ·»åŠ  LoRA åˆå¹¶è„šæœ¬ã€Dockerfileã€æˆ–æŠŠ README æ¨é€åˆ°è¿œç¨‹ä»“åº“ï¼Œæˆ‘å¯ä»¥ç»§ç»­ååŠ©ã€‚ï¼ˆFull implementation and details are in `FineTuning.py`. ï¼‰
